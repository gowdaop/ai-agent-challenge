"""
agent.py - FIXED VERSION with better indentation handling
"""

import os
import sys
import argparse
from typing import Dict, List, Any, TypedDict, Literal
from pathlib import Path
import pandas as pd
import traceback
import importlib.util
import json
import re

from groq import Groq
from langgraph.graph import StateGraph, END


class AgentState(TypedDict):
    """Enhanced state with evaluation feedback"""
    target_bank: str
    sample_pdf_path: str
    sample_csv_path: str
    
    # Planning phase
    table_preview: str
    expected_df: Any
    column_mapping: Dict[str, Any]
    
    # Generation phase
    generated_code: str
    
    # Evaluation phase
    test_results: Dict[str, Any]
    evaluation_feedback: str
    parsed_df: Any
    
    # Iteration control
    iteration: int
    max_iterations: int
    success: bool
    errors: List[Dict[str, str]]
    
    # Transparency
    reasoning_log: List[str]


PARSER_TEMPLATE = '''"""
Auto-generated parser for {bank_name} bank statements
Generated by: Parser Generator Agent
"""
import pandas as pd
import pdfplumber
from typing import Dict, List


def parse(pdf_path: str) -> pd.DataFrame:
    """
    Parse {bank_name} bank statement PDF and extract transactions
    
    Args:
        pdf_path: Path to PDF file
        
    Returns:
        DataFrame with columns: Date, Description, Debit Amt, Credit Amt, Balance
    """
    try:
{parsing_logic}
        
        if not all_data:
            return pd.DataFrame(columns=['Date', 'Description', 'Debit Amt', 'Credit Amt', 'Balance'])
        
        # Create DataFrame
        df = pd.DataFrame(all_data)
        
        # Ensure correct column order
        df = df[['Date', 'Description', 'Debit Amt', 'Credit Amt', 'Balance']]
        
        # Clean and convert numeric columns
        for col in ['Debit Amt', 'Credit Amt', 'Balance']:
            df[col] = pd.to_numeric(
                df[col].astype(str).str.replace(',', '').str.strip(), 
                errors='coerce'
            ).fillna(0)
        
        # Clean string columns
        df['Date'] = df['Date'].astype(str).str.strip()
        df['Description'] = df['Description'].astype(str).str.strip()
        
        return df
    
    except Exception as e:
        print(f"Parser error: {{e}}")
        return pd.DataFrame(columns=['Date', 'Description', 'Debit Amt', 'Credit Amt', 'Balance'])
'''


class EnhancedParserAgent:
    """
    Parser generator using Evaluator-Optimizer pattern from Anthropic's guide
    """
    
    def __init__(self, api_key: str, model: str = "llama-3.3-70b-versatile"):
        self.client = Groq(api_key=api_key)
        self.model = model
        self.workflow = self._create_workflow_v2()
    
    def _call_llm(self, prompt: str, system_prompt: str = None) -> str:
        """Call LLM with optional system prompt"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.1,
            max_tokens=4096
        )
        return completion.choices[0].message.content
    
    def _create_workflow_v2(self):
        """Enhanced workflow with column detection step"""
        workflow = StateGraph(AgentState)
        
        # Nodes
        workflow.add_node("analyze", self.analyze_structure)
        workflow.add_node("detect_columns", self.detect_column_mapping)  # NEW
        workflow.add_node("plan", self.plan_extraction)
        workflow.add_node("generate", self.generate_parser_v2)  # Use v2
        workflow.add_node("llm_correct", self.llm_correct_parser)
        workflow.add_node("validate", self.validate_syntax)
        workflow.add_node("execute", self.execute_and_evaluate)
        workflow.add_node("save", self.save_parser)
        
        # Flow
        workflow.set_entry_point("analyze")
        workflow.add_edge("analyze", "detect_columns")  # NEW
        workflow.add_edge("detect_columns", "plan")     # NEW
        workflow.add_edge("plan", "generate")
        workflow.add_edge("generate", "validate")
        workflow.add_edge("validate", "execute")
        
        # Use improved routing
        workflow.add_conditional_edges("execute", self.route_decision_v2, {
            "regenerate": "generate",
            "llm_fix": "llm_correct",
            "save": "save",
            "fail": END
        })
        
        workflow.add_edge("llm_correct", "validate")
        workflow.add_edge("save", END)
        
        return workflow.compile()
        
    def analyze_structure(self, state: AgentState) -> AgentState:
        """
        Step 1: Analyze PDF structure and extract table information
        """
        print(f"\n{'='*70}\n[ANALYZING] STEP 1: ANALYZING PDF STRUCTURE\n{'='*70}")
        
        state['reasoning_log'] = []
        state['reasoning_log'].append("Starting PDF structure analysis...")
        
        import pdfplumber
        
        # Extract comprehensive table information
        tables_info = []
        with pdfplumber.open(state['sample_pdf_path']) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                if tables:
                    print(f"  Page {page_num + 1}: {len(tables)} table(s) found")
                    
                    for table_idx, table in enumerate(tables):
                        if table and len(table) > 1:
                            tables_info.append({
                                'page': page_num + 1,
                                'table_idx': table_idx,
                                'rows': len(table),
                                'cols': len(table[0]) if table else 0,
                                'header': table[0] if table else None,
                                'sample_rows': table[1:4] if len(table) > 1 else []
                            })
        
        # Store structured preview
        if tables_info:
            state['table_preview'] = json.dumps(tables_info, indent=2)
            total_rows = sum(t['rows'] for t in tables_info)
            state['reasoning_log'].append(
                f"Found {len(tables_info)} table(s) with {total_rows} total rows"
            )
        else:
            state['table_preview'] = "No tables detected"
            state['reasoning_log'].append("WARNING: No tables found in PDF")
        
        # Load expected output
        expected_df = pd.read_csv(state['sample_csv_path'])
        state['expected_df'] = expected_df
        state['iteration'] = 0
        state['errors'] = []
        state['success'] = False
        
        print(f"  [OK] Expected transactions: {len(expected_df)}")
        print(f"  [OK] Expected columns: {list(expected_df.columns)}")
        
        state['reasoning_log'].append(f"Target: Extract {len(expected_df)} transactions")
        
        return state  
    
    def plan_extraction(self, state: AgentState) -> AgentState:
        """
        Step 2: Create extraction plan using LLM reasoning
        """
        print(f"\n{'='*70}\n[PLANNING] STEP 2: PLANNING EXTRACTION STRATEGY\n{'='*70}")
        
        expected_df = state['expected_df']
        
        # JSON example as separate string (not in f-string)
        json_example = '''{
        "target_tables": ["all"],
        "header_detection": "skip first row of each table",
        "column_mapping": {
            "0": "Date",
            "1": "Description",
            "2": "Debit Amt",
            "3": "Credit Amt",
            "4": "Balance"
        },
        "cleaning_steps": ["remove commas from numbers", "handle empty cells"]
    }'''
        
        # Now build prompt with the example
        planning_prompt = f"""You are an expert at analyzing PDF tables and planning data extraction.

    TASK: Create a detailed extraction plan for bank statement data.

    PDF STRUCTURE:
    {state['table_preview']}

    EXPECTED OUTPUT (first 3 rows):
    {expected_df.head(3).to_string()}

    REQUIRED COLUMNS: {list(expected_df.columns)}

    Analyze the PDF structure and provide:
    1. Which table(s) contain transaction data
    2. How to identify the header row vs data rows
    3. Column mapping (PDF column index ‚Üí output column name)
    4. Data cleaning steps needed

    Output ONLY valid JSON in this format:
    {json_example}

    Generate your extraction plan as valid JSON. No markdown, no code blocks."""
        
        try:
            plan_response = self._call_llm(
                planning_prompt,
                system_prompt="You are a data extraction planning expert. Output ONLY valid JSON, no markdown."
            )            
            # Extract JSON from response            
            plan_text = plan_response.strip()
            if "```json" in plan_text:
                plan_text = plan_text.split("```json")[1].split("```")[0]
            elif "```" in plan_text:
                plan_text = plan_text.split("``````")[0]
            
            plan_text = plan_text.strip()
            
            # Parse JSON
            plan = json.loads(plan_text)
            state['column_mapping'] = plan
            
            print("  [OK] Extraction plan created:")
            print(f"    - Target tables: {plan.get('target_tables', 'all')}")
            print(f"    - Columns mapped: {len(plan.get('column_mapping', {}))}")
            
            state['reasoning_log'].append("Created extraction plan")
            
        except json.JSONDecodeError as e:
            print(f"  [WARNING] JSON parsing failed: {e}")
            print(f"  Raw response (first 200 chars): {plan_response[:200]}")
            state['column_mapping'] = {"fallback": True}
            state['reasoning_log'].append("Using fallback extraction strategy")
        except Exception as e:
            print(f"  [WARNING] Planning failed, using default strategy: {e}")
            state['column_mapping'] = {"fallback": True}
            state['reasoning_log'].append("Using fallback extraction strategy")
        
        return state

    def detect_column_mapping(self, state: AgentState) -> AgentState:
        """
        NEW STEP: Empirically detect correct column mapping before code generation
        This prevents 2-3 wasted attempts on wrong mappings
        """
        print(f"\n{'='*70}\n[DETECTING] Empirical Column Mapping Detection\n{'='*70}")
        
        import pdfplumber
        import pandas as pd
        
        expected = state['expected_df']
        
        # Test all reasonable column combinations
        test_cases = [
            (2, 3, "Standard: Debit=col2, Credit=col3"),
            (3, 2, "Swapped: Debit=col3, Credit=col2"),
            (2, 4, "Alt1: Debit=col2, Credit=col4"),
            (3, 4, "Alt2: Debit=col3, Credit=col4"),
        ]
        
        best_match = None
        best_score = 0
        
        with pdfplumber.open(state['sample_pdf_path']) as pdf:
            # Extract first page to test
            tables = pdf.pages[0].extract_tables()
            if not tables or not tables[0] or len(tables[0]) < 2:
                print("  [WARNING] Could not detect columns, using defaults")
                state['column_mapping']['detected'] = {'debit_idx': 2, 'credit_idx': 3}
                return state
            
            sample_table = tables[0]
            sample_rows = sample_table[1:min(6, len(sample_table))]  # Test first 5 data rows
            
            for debit_idx, credit_idx, label in test_cases:
                # Extract test values
                test_debits = []
                test_credits = []
                
                for row in sample_rows:
                    if row and len(row) > max(debit_idx, credit_idx):
                        debit_val = str(row[debit_idx] or '').replace(',', '').strip()
                        credit_val = str(row[credit_idx] or '').replace(',', '').strip()
                        
                        try:
                            test_debits.append(float(debit_val) if debit_val else 0)
                            test_credits.append(float(credit_val) if credit_val else 0)
                        except ValueError:
                            continue
                
                if not test_debits or not test_credits:
                    continue
                
                # Compare with expected (first few rows)
                expected_debits = expected['Debit Amt'].head(len(test_debits)).values
                expected_credits = expected['Credit Amt'].head(len(test_credits)).values
                
                # Calculate match score
                debit_matches = sum(1 for i in range(len(test_debits)) 
                                if abs(test_debits[i] - expected_debits[i]) < 0.01)
                credit_matches = sum(1 for i in range(len(test_credits)) 
                                if abs(test_credits[i] - expected_credits[i]) < 0.01)
                
                score = debit_matches + credit_matches
                
                print(f"  {label}: {score}/{len(test_debits)*2} matches")
                
                if score > best_score:
                    best_score = score
                    best_match = {'debit_idx': debit_idx, 'credit_idx': credit_idx, 'label': label}
        
        if best_match and best_score > 0:
            state['column_mapping']['detected'] = best_match
            print(f"  [SUCCESS] Detected: {best_match['label']}")
        else:
            state['column_mapping']['detected'] = {'debit_idx': 2, 'credit_idx': 3, 'label': 'Default'}
            print(f"  [WARNING] Using default mapping")
        
        return state
    def generate_parser_v2(self, state: AgentState) -> AgentState:
        """Enhanced generation with pre-detected column mapping"""
        attempt = state['iteration'] + 1
        print(f"\n{'='*70}\n[GENERATING] STEP 3: GENERATING PARSER (Attempt {attempt})\n{'='*70}")
        
        expected_sample = state['expected_df'].head(3).to_string()
        prev_feedback = state.get('evaluation_feedback', '')
        
        # Get detected mapping
        detected = state['column_mapping'].get('detected', {})
        debit_idx = detected.get('debit_idx', 2)
        credit_idx = detected.get('credit_idx', 3)
        
        # More explicit prompt with exact column indices
        prompt = f"""Generate a bank statement PDF parser function.

    PDF STRUCTURE:
    {state['table_preview']}

    EXPECTED OUTPUT (first 3 rows):
    {expected_sample}

    DETECTED COLUMN MAPPING (use these exact indices):
    - Date: row[0]
    - Description: row[1]  
    - Debit Amt: row[{debit_idx}]  ‚Üê USE INDEX {debit_idx}
    - Credit Amt: row[{credit_idx}]  ‚Üê USE INDEX {credit_idx}
    - Balance: row[4]

    {prev_feedback}

    MANDATORY REQUIREMENTS:
    1. Function signature: def parse(pdf_path: str) -> pd.DataFrame
    2. Use: with pdfplumber.open(pdf_path) as pdf:  ‚Üê MUST use pdf_path parameter
    3. Loop: for page in pdf.pages:
    4. Loop: for table in page.extract_tables():
    5. Skip header: for row in table[1:]:
    6. Use exact column indices shown above
    7. Handle None: row[i] or ''

    COMPLETE WORKING CODE (fill in the logic):
    ```python
    import pandas as pd
    import pdfplumber

    def parse(pdf_path: str) -> pd.DataFrame:
        all_data = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                tables = page.extract_tables()
                for table in tables:
                    if not table or len(table) < 2:
                        continue
                    for row in table[1:]:
                        if row and len(row) >= 5:
                            all_data.append({{
                                'Date': row[0] or '',
                                'Description': row[1] or '',
                                'Debit Amt': row[{debit_idx}] or '',
                                'Credit Amt': row[{credit_idx}] or '',
                                'Balance': row[4] or ''
                            }})
        
        df = pd.DataFrame(all_data)
        for col in ['Debit Amt', 'Credit Amt', 'Balance']:
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
        return df[['Date', 'Description', 'Debit Amt', 'Credit Amt', 'Balance']]
    ```

    Output this EXACT code with correct column indices {debit_idx} and {credit_idx}."""

        try:
            response = self._call_llm(
                prompt,
                system_prompt="Output ONLY the complete Python code. Do not change column indices. Use pdf_path parameter."
            )
            
            # Extract and validate code
            code = response.strip()
            if "```python" in code:
                code = code.split("```python", 1)[1].split("```", 1)[0]
            elif "```" in code:
                code = code.split("```", 1)[1].split("```", 1)[0]
            code = code.strip()
            
            # Critical validation
            if "pdfplumber.open(pdf_path)" not in code:
                print(f"  [ERROR] Generated code doesn't use pdf_path parameter")
                raise ValueError("Must use: pdfplumber.open(pdf_path)")
            
            if "def parse" not in code:
                raise ValueError("Missing parse function")
            
            # Ensure imports
            if "import pandas" not in code:
                code = "import pandas as pd\n" + code
            if "import pdfplumber" not in code:
                code = "import pdfplumber\n" + code
            
            state['generated_code'] = code
            print(f"  [OK] Generated parser ({len(code)} chars)")
            Path("_debug_generated.py").write_text(code)
            state['reasoning_log'].append(f"Parser generated with detected indices (attempt {attempt})")
            
        except Exception as e:
            print(f"  [ERROR] Generation failed: {e}")
            state['errors'].append({"step": "generate", "error": str(e)})
            state['generated_code'] = ""
        
        return state

    def _extract_and_indent_logic(self, text: str) -> str:
        """
        FIXED: Properly extract and indent code while preserving structure
        """
        # Remove markdown code blocks
        if "```python" in text:
            code = text.split("```python")[1].split("```")[0]
        elif "```" in text:
            code = text.split("```")[1].split("```")[0]
        else:
            code = text
        
        lines = code.strip().split('\n')
        
        # Find the start line (where all_data appears)
        start_idx = 0
        for i, line in enumerate(lines):
            if 'all_data' in line or 'with pdfplumber' in line:
                start_idx = i
                break
        
        # Get relevant lines
        code_lines = lines[start_idx:]
        
        if not code_lines:
            return ""
        
        # Find minimum indentation (excluding empty lines)
        non_empty_lines = [line for line in code_lines if line.strip()]
        if not non_empty_lines:
            return ""
        
        min_indent = min(len(line) - len(line.lstrip()) for line in non_empty_lines)
        
        # Re-indent: remove common indentation, then add 8 spaces base
        result = []
        for line in code_lines:
            if line.strip():  # Non-empty line
                # Remove the minimum indentation
                dedented = line[min_indent:] if len(line) > min_indent else line
                # Add 8-space base indentation
                result.append('        ' + dedented)
            else:  # Empty line
                result.append('')
        
        final_code = '\n'.join(result)
        
        # Validation: check for basic structure
        if 'with pdfplumber' not in final_code or 'all_data' not in final_code:
            raise ValueError("Generated code missing required structure")
        
        return final_code
    
    def _validate_generated_code(self, code: str) -> tuple[bool, list]:
        """
        Validate generated code for common issues
        Returns: (is_valid, list_of_issues)
        """
        issues = []
        
        # Check 1: No hardcoded paths
        if re.search(r"pdfplumber\.open\(['\"].*?['\"]\)", code):
            issues.append("Contains hardcoded file path in pdfplumber.open()")
        
        # Check 2: Must use pdf_path parameter
        if "pdfplumber.open(pdf_path)" not in code:
            issues.append("Missing 'pdfplumber.open(pdf_path)' - must use the parameter")
        
        # Check 3: Must initialize all_data
        if "all_data = []" not in code:
            issues.append("Missing 'all_data = []' initialization")
        
        # Check 4: Must have pdfplumber import context
        if "with pdfplumber.open" not in code:
            issues.append("Missing 'with pdfplumber.open' statement")
        
        return len(issues) == 0, issues

    def _detect_correct_mapping(self, state: AgentState) -> dict:
        """
        Detect Debit/Credit column indices by testing against COMPLETE expected data
        """
        import pdfplumber
        import pandas as pd

        expected = pd.read_csv(state['sample_csv_path'])
        
        # Extract ALL data from PDF, not just first page
        def extract_with_mapping(debit_i, credit_i):
            all_data = []
            with pdfplumber.open(state['sample_pdf_path']) as pdf:
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2:
                            continue
                        for row in table[1:]:  # Skip header
                            if row and len(row) >= 5:
                                all_data.append({
                                    'Date': str(row[0] or '').strip(),
                                    'Description': str(row[1] or '').strip(),
                                    'Debit Amt': str(row[debit_i] or '').strip() if debit_i < len(row) else '',
                                    'Credit Amt': str(row[credit_i] or '').strip() if credit_i < len(row) else '',
                                    'Balance': str(row[4] or '').strip() if len(row) > 4 else ''
                                })
            
            if not all_data:
                return None
            
            df = pd.DataFrame(all_data)
            
            # Clean numeric columns
            for col in ['Debit Amt', 'Credit Amt']:
                df[col] = pd.to_numeric(
                    df[col].str.replace(',', ''),
                    errors='coerce'
                ).fillna(0)
            
            return df
        
        # Test different column combinations on FULL dataset
        test_combinations = [
            (2, 3, "standard"),
            (3, 2, "swapped"),
            (2, 4, "alt1"),
            (3, 4, "alt2"),
            (4, 2, "alt3"),
            (4, 3, "alt4"),
        ]
        
        best_score = -1
        best_mapping = None
        
        print(f"  [EMPIRICAL] Testing column combinations on FULL dataset...")
        
        for debit_i, credit_i, label in test_combinations:
            test_df = extract_with_mapping(debit_i, credit_i)
            
            if test_df is None or len(test_df) == 0:
                continue
            
            # Compare with expected - handle different lengths
            min_len = min(len(test_df), len(expected))
            
            # Compare Debit and Credit columns
            debit_matches = (
                test_df['Debit Amt'].head(min_len).values == 
                expected['Debit Amt'].head(min_len).values
            ).sum()
            
            credit_matches = (
                test_df['Credit Amt'].head(min_len).values == 
                expected['Credit Amt'].head(min_len).values
            ).sum()
            
            total_matches = debit_matches + credit_matches
            total_possible = min_len * 2
            accuracy = (total_matches / total_possible * 100) if total_possible > 0 else 0
            
            print(f"    {label} (D={debit_i}, C={credit_i}): {total_matches}/{total_possible} matches ({accuracy:.1f}%)")
            
            if total_matches > best_score:
                best_score = total_matches
                best_mapping = {
                    'debit_idx': debit_i,
                    'credit_idx': credit_i,
                    'strategy': f'empirical_{label}',
                    'accuracy': accuracy
                }
        
        if best_mapping and best_mapping['accuracy'] > 80:
            print(f"  [SUCCESS] Found mapping: {best_mapping['strategy']} with {best_mapping['accuracy']:.1f}% accuracy")
            return best_mapping
        elif best_mapping:
            print(f"  [WARNING] Best mapping only {best_mapping['accuracy']:.1f}% accurate: {best_mapping['strategy']}")
            return best_mapping
        else:
            print(f"  [ERROR] No valid mapping found")
            return {'debit_idx': 2, 'credit_idx': 3, 'strategy': 'fallback', 'accuracy': 0}

    def llm_correct_parser(self, state: AgentState) -> AgentState:
        """
        Step 3b: Use LLM to fix the parser when automatic detection fails
        """
        print(f"\n{'='*70}\n[LLM CORRECTION] Asking LLM to fix the parser\n{'='*70}")
        
        # Prepare diagnostic information - handle None/missing values
        parsed_df = state.get('parsed_df')
        if parsed_df is not None and not parsed_df.empty:
            parsed_sample = parsed_df.head(3).to_string()
            parsed_rows = len(parsed_df)
        else:
            parsed_sample = "No data extracted"
            parsed_rows = 0
        
        expected_sample = state['expected_df'].head(3).to_string()
        
        correction_prompt = f"""You are debugging a bank statement PDF parser that extracts transaction data.

    CURRENT PARSER CODE:
    {state['generated_code']}

    PROBLEM:
    The parser extracts {parsed_rows} rows but produces incorrect values.

    PARSED OUTPUT (first 3 rows):
    {parsed_sample}

    EXPECTED OUTPUT (first 3 rows):
    {expected_sample}

    EVALUATION ISSUES:
    {state.get('evaluation_feedback', 'No specific feedback')}

    PDF STRUCTURE:
    {state['table_preview']}

    TASK: Generate ONLY the parsing logic (the code inside the try block) that correctly extracts:
    - Date from correct column
    - Description from correct column  
    - Debit Amt from correct column
    - Credit Amt from correct column
    - Balance from correct column

    Requirements:
    1. Start with: all_data = []
    2. Use: with pdfplumber.open(pdf_path) as pdf:
    3. Iterate through pages and tables correctly
    4. Use correct column indices based on the expected output above
    5. Handle empty/None values with: row[i] or ''
    6. Ensure proper indentation (8 spaces base, +4 for each nesting level)

    Output ONLY the parsing logic code, no explanations, no markdown blocks."""

        try:
            llm_response = self._call_llm(
                correction_prompt,
                system_prompt="You are a Python expert. Output ONLY executable Python code, no markdown, no explanations."
            )
            
            # Extract code
            logic = self._extract_and_indent_logic(llm_response)
            
            if not logic or len(logic.strip()) < 50:
                raise ValueError("LLM generated code too short")
            
            # Build complete parser
            full_code = PARSER_TEMPLATE.format(
                bank_name=state['target_bank'].upper(),
                parsing_logic=logic
            )
            
            state['generated_code'] = full_code
            print(f"  [OK] LLM corrected code generated ({len(logic)} chars)")
            
            # Save for debugging
            Path("_debug_llm_corrected.py").write_text(full_code)
            state['reasoning_log'].append(f"LLM correction applied (attempt {state['iteration']})")
            
        except Exception as e:
            print(f"  [ERROR] LLM correction failed: {e}")
            state['errors'].append({"step": "llm_correct", "error": str(e)})
        
        return state

    def validate_syntax(self, state: AgentState) -> AgentState:
        """
        Step 4: Validate code syntax before execution
        """
        print(f"\n{'='*70}\n[VALIDATING] STEP 4: VALIDATING SYNTAX\n{'='*70}")
        
        if not state['generated_code']:
            state['errors'].append({"step": "validate", "error": "No code generated"})
            return state
        
        try:
            compile(state['generated_code'], '<string>', 'exec')
            print("  [OK] Syntax valid")
            state['reasoning_log'].append("Code syntax validation passed")
            # CRITICAL: Clear previous validation errors when syntax becomes valid
            state['errors'] = [e for e in state['errors'] if e.get('step') != 'validate']
            
        except SyntaxError as e:
            error_msg = f"Syntax error at line {e.lineno}: {e.msg}"
            print(f"  [ERROR] {error_msg}")
            
            # Show the problematic line with context
            lines = state['generated_code'].split('\n')
            if e.lineno and 0 < e.lineno <= len(lines):
                start = max(0, e.lineno - 3)
                end = min(len(lines), e.lineno + 2)
                print(f"\n  Code context (lines {start+1}-{end}):")
                for i in range(start, end):
                    marker = "  >>> " if i == e.lineno - 1 else "      "
                    print(f"{marker}Line {i+1}: {lines[i]}")
            
            # Save full error for debugging
            debug_file = Path("_debug_syntax_error.txt")
            debug_file.write_text(
                f"Syntax Error at line {e.lineno}: {e.msg}\n"
                f"Offset: {e.offset}\n\n"
                f"Full generated code:\n{state['generated_code']}"
            )
            print(f"  [DEBUG] Full code saved to {debug_file}")
            
            state['errors'].append({"step": "validate", "error": error_msg})
            
            # Generate actionable feedback
            state['evaluation_feedback'] = self._generate_syntax_feedback(e, lines)
        
        return state

    def _generate_syntax_feedback(self, e: SyntaxError, lines: List[str]) -> str:
        """Generate specific feedback based on the syntax error"""
        
        problematic_line = lines[e.lineno-1] if e.lineno and 0 < e.lineno <= len(lines) else 'N/A'
        
        # Detect specific error patterns
        feedback_parts = [
            f"SYNTAX ERROR at line {e.lineno}: {e.msg}",
            f"\nProblematic line:\n{problematic_line}",
            "\nLikely causes:"
        ]
        
        # Pattern detection
        if "unexpected indent" in e.msg:
            feedback_parts.append("- Line has incorrect indentation (too many or too few spaces)")
            feedback_parts.append("- Previous line might be missing a colon (:)")
            feedback_parts.append("- Mixed tabs and spaces")
            feedback_parts.append("\nFIX: Ensure consistent 4-space indentation at each nesting level")
            
        elif "invalid syntax" in e.msg:
            if e.lineno > 1:
                prev_line = lines[e.lineno-2]
                if not prev_line.rstrip().endswith(':') and any(kw in prev_line for kw in ['if', 'for', 'while', 'with', 'def', 'class']):
                    feedback_parts.append(f"- Previous line missing colon: {prev_line}")
            feedback_parts.append("- Unclosed bracket, parenthesis, or quote")
            feedback_parts.append("- Missing colon after if/for/while/with/def")
            
        elif "expected an indented block" in e.msg:
            feedback_parts.append("- Empty block after control statement")
            feedback_parts.append("- Need to add code or use 'pass' statement")
            feedback_parts.append("\nFIX: Add content after the colon or use 'pass'")
        
        else:
            feedback_parts.append("- Check for missing colons, unclosed brackets, or quotes")
            feedback_parts.append("- Verify indentation is consistent (4 spaces per level)")
        
        feedback_parts.append("\n\nCORRECT INDENTATION EXAMPLE:")
        feedback_parts.append("def parse(pdf_path: str):")
        feedback_parts.append("    try:")
        feedback_parts.append("        all_data = []")
        feedback_parts.append("        with pdfplumber.open(pdf_path) as pdf:")
        feedback_parts.append("            for page in pdf.pages:")
        feedback_parts.append("                tables = page.extract_tables()")
        
        return '\n'.join(feedback_parts)
    
    def execute_and_evaluate(self, state: AgentState) -> AgentState:
        """
        Step 5: Execute code and evaluate results
        """
        print(f"\n{'='*70}\n[TESTING] STEP 5: EXECUTING AND EVALUATING\n{'='*70}")
        
        state['iteration'] += 1
        
        # Skip execution if there were validation errors
        if state['errors'] and any(e.get('step') == 'validate' for e in state['errors']):
            print("  [SKIPPING] Skipping execution due to validation errors")
            return state
        
        if not state['generated_code']:
            return state
        
        temp_file = Path("_temp_parser.py")
        try:
            # Write and import module
            temp_file.write_text(state['generated_code'])
            
            spec = importlib.util.spec_from_file_location("_temp", temp_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            # Execute parser
            parsed_df = module.parse(state['sample_pdf_path'])
            state['parsed_df'] = parsed_df
            expected_df = state['expected_df']
            
            print(f"  Extracted: {len(parsed_df)} rows")
            print(f"  Expected: {len(expected_df)} rows")
            
            # Detailed evaluation
            evaluation = self._evaluate_results(parsed_df, expected_df)
            state['test_results'] = evaluation
            
            if evaluation['success']:
                print(f"\n  [SUCCESS] SUCCESS! Parser works correctly")
                state['success'] = True
                state['reasoning_log'].append("Parser validation successful")
            else:
                # Generate detailed feedback for next iteration
                feedback = self._generate_feedback(evaluation, parsed_df, expected_df)
                state['evaluation_feedback'] = feedback
                
                print(f"\n  [ERROR] Issues found:")
                for issue in evaluation.get('issues', [])[:3]:  # Show first 3
                    print(f"     - {issue}")
            
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            print(f"  [ERROR] Execution error: {error_msg}")
            
            state['errors'].append({"step": "execute", "error": error_msg})
            
            # Get traceback details
            tb_lines = traceback.format_exc().split('\n')
            relevant_tb = [line for line in tb_lines if '_temp_parser.py' in line or 'parse' in line]
            
            state['evaluation_feedback'] = f"""
EXECUTION ERROR:
{error_msg}

Relevant traceback:
{chr(10).join(relevant_tb[:5])}

Common fixes:
- Check if len(row) >= expected_columns before accessing row[index]
- Handle None/empty values: use (row[i] or '') instead of row[i]
- Validate table exists before processing: if not table or len(table) < 2: continue
- Check array bounds before accessing elements
"""
        finally:
            if temp_file.exists():
                temp_file.unlink()
        
        return state
    
    def _evaluate_results(self, parsed: pd.DataFrame, expected: pd.DataFrame) -> Dict:
        """Comprehensive result evaluation"""
        issues = []
        
        # Check row count
        if len(parsed) != len(expected):
            issues.append(f"Row count mismatch: got {len(parsed)}, expected {len(expected)}")
        
        # Check columns
        missing_cols = set(expected.columns) - set(parsed.columns)
        if missing_cols:
            issues.append(f"Missing columns: {missing_cols}")
        
        # Check data match
        if len(parsed) == len(expected) and set(parsed.columns) == set(expected.columns):
            # Compare each column
            for col in expected.columns:
                if not parsed[col].equals(expected[col]):
                    # Count mismatches
                    if pd.api.types.is_numeric_dtype(expected[col]):
                        mismatch = (parsed[col] != expected[col]).sum()
                    else:
                        mismatch = (parsed[col].astype(str) != expected[col].astype(str)).sum()
                    
                    if mismatch > 0:
                        issues.append(f"Column '{col}': {mismatch} mismatched values")
        
        return {
            'success': len(issues) == 0,
            'issues': issues,
            'parsed_rows': len(parsed),
            'expected_rows': len(expected)
        }
    
    def _generate_feedback(self, evaluation: Dict, parsed: pd.DataFrame, expected: pd.DataFrame) -> str:
        """Generate detailed feedback for code improvement"""
        feedback = ["EVALUATION RESULTS:"]
        
        for issue in evaluation['issues'][:5]:
            feedback.append(f"  ‚ùå {issue}")
        
        feedback.append("\nDETAILED ANALYSIS:")
        
        # Column-specific comparison for swapped columns
        if 'Debit Amt' in parsed.columns and 'Credit Amt' in parsed.columns:
            feedback.append("\nCOLUMN MISMATCH DETECTED:")
            feedback.append("\nFirst 3 rows - YOUR OUTPUT vs EXPECTED:")
            
            for idx in range(min(3, len(parsed), len(expected))):
                feedback.append(f"\nRow {idx+1}:")
                feedback.append(f"  Your Debit: {parsed.iloc[idx]['Debit Amt']}, Credit: {parsed.iloc[idx]['Credit Amt']}")
                feedback.append(f"  Expected Debit: {expected.iloc[idx]['Debit Amt']}, Credit: {expected.iloc[idx]['Credit Amt']}")
            
            # Check if columns are swapped
            debit_matches_credit = (parsed['Debit Amt'].fillna(0) == expected['Credit Amt'].fillna(0)).sum()
            credit_matches_debit = (parsed['Credit Amt'].fillna(0) == expected['Debit Amt'].fillna(0)).sum()
            
            if debit_matches_credit > 40 and credit_matches_debit > 40:
                feedback.append("\n‚ö†Ô∏è COLUMNS ARE SWAPPED!")
                feedback.append("Your Debit column matches expected Credit column")
                feedback.append("Your Credit column matches expected Debit column")
                feedback.append("\nFIX: Swap the column mapping:")
                feedback.append("  'Debit Amt': row[3] or '',  # ‚Üê Try row[3] instead of row[2]")
                feedback.append("  'Credit Amt': row[2] or '',  # ‚Üê Try row[2] instead of row[3]")
        
        # Row count analysis (keep existing logic)
        if evaluation['parsed_rows'] < evaluation['expected_rows']:
            missing = evaluation['expected_rows'] - evaluation['parsed_rows']
            feedback.append(f"\n- Missing {missing} rows. Check:")
            feedback.append(f"  ‚Ä¢ Process ALL pages")
            feedback.append(f"  ‚Ä¢ Process ALL tables")
        elif evaluation['parsed_rows'] > evaluation['expected_rows']:
            extra = evaluation['parsed_rows'] - evaluation['expected_rows']
            feedback.append(f"\n- Extra {extra} rows. Check:")
            feedback.append(f"  ‚Ä¢ Skip header rows with table[1:]")
        
        feedback.append(f"\nACTION: Fix column mapping to match expected output.")
        
        return '\n'.join(feedback)

    
    def save_parser(self, state: AgentState) -> AgentState:
        """Step 6: Save successful parser"""
        print(f"\n{'='*70}\n[SAVING] STEP 6: SAVING PARSER\n{'='*70}")
        
        out_dir = Path("custom_parsers")
        out_dir.mkdir(exist_ok=True)
        out_file = out_dir / f"{state['target_bank']}_parser.py"
        out_file.write_text(state['generated_code'])
        
        print(f"  [SUCCESS] Saved to: {out_file}")
        print(f"  [ANALYZING] Iterations: {state['iteration']}")
        
        # Save reasoning log
        log_file = out_dir / f"{state['target_bank']}_generation_log.txt"
        log_file.write_text('\n'.join(state['reasoning_log']))
        print(f"  [LOG] Log saved: {log_file}")
        
        return state
    
    def route_decision_v2(self, state: AgentState) -> str:
        """Smarter routing with earlier LLM intervention"""
        if state.get('success'):
            return "save"
        
        if state['iteration'] >= state['max_iterations']:
            print(f"\n[ERROR] Max iterations reached")
            return "fail"
        
        # Check error patterns
        recent_errors = state.get('errors', [])[-3:]  # Last 3 errors
        
        # If same error repeats, trigger LLM fix immediately
        if len(recent_errors) >= 2:
            error_types = [e.get('error', '') for e in recent_errors]
            if len(set(error_types)) == 1:  # Same error repeated
                print(f"\n[EARLY LLM FIX] Repeated error detected, using LLM correction")
                return "llm_fix"
        
        # If column mismatch detected, trigger empirical fix
        feedback = state.get('evaluation_feedback', '')
        if 'COLUMNS ARE SWAPPED' in feedback and state['iteration'] >= 2:
            print(f"\n[EMPIRICAL FIX] Column swap detected, re-detecting mapping")
            # Update mapping and regenerate
            return "regenerate"
        
        # Standard LLM fix after attempt 3
        if state['iteration'] == 3:
            print(f"\n[LLM FIX] Standard LLM correction at attempt 3")
            return "llm_fix"
        
        print(f"\n[RETRYING] Attempt {state['iteration'] + 1}/{state['max_iterations']}")
        return "regenerate"

    
    def run(self, target_bank: str, max_iterations: int = 5) -> bool:
        """Execute the agent workflow"""
        initial_state = AgentState(
            target_bank=target_bank,
            sample_pdf_path=f"data/{target_bank}/{target_bank}_sample.pdf",
            sample_csv_path=f"data/{target_bank}/result.csv",
            generated_code="",
            test_results={},
            iteration=0,
            max_iterations=max_iterations,
            success=False,
            table_preview="",
            column_mapping={},
            expected_df=None,
            parsed_df=None,
            errors=[],
            evaluation_feedback="",
            reasoning_log=[]
        )
        
        try:
            final_state = self.workflow.invoke(
                initial_state,
                config={"recursion_limit": max_iterations * 10}
            )
            return final_state.get('success', False)
        except Exception as e:
            print(f"\n‚ùå Workflow error: {e}")
            traceback.print_exc()
            return False


def main():
    parser = argparse.ArgumentParser(
        description="Enhanced Parser Generator - FIXED INDENTATION"
    )
    parser.add_argument("--target", required=True, help="Target bank name (e.g., icici)")
    parser.add_argument("--api-key", default=os.getenv("GROQ_API_KEY"), help="Groq API key")
    parser.add_argument("--max-attempts", type=int, default=3, help="Max generation attempts")
    parser.add_argument("--model", default="llama-3.3-70b-versatile", help="Groq model to use")
    
    args = parser.parse_args()
    
    if not args.api_key:
        print("‚ùå Error: GROQ_API_KEY not set")
        print("   Set it: export GROQ_API_KEY='your-key-here'")
        sys.exit(1)
    
    # Get the directory of the script
    script_dir = Path(__file__).parent.resolve()
    
    # Validate input files exist
    pdf_path = script_dir / f"data/{args.target}/{args.target}_sample.pdf"
    csv_path = script_dir / f"data/{args.target}/result.csv"
    
    if not pdf_path.exists():
        print(f"‚ùå Error: PDF not found: {pdf_path}")
        sys.exit(1)
    
    if not csv_path.exists():
        print(f"‚ùå Error: CSV not found: {csv_path}")
        sys.exit(1)
    
    print(f"\n{'='*70}")
    print(f"üöÄ ENHANCED PARSER GENERATOR (FIXED)")
    print(f"{'='*70}")
    print(f"  Target: {args.target}")
    print(f"  Model: {args.model}")
    print(f"  Max attempts: {args.max_attempts}")
    print(f"{'='*70}\n")
    
    try:
        agent = EnhancedParserAgent(args.api_key, args.model)
        success = agent.run(args.target, args.max_attempts)
        
        if success:
            print(f"\n{'='*70}")
            print(f"‚úÖ SUCCESS!")
            print(f"{'='*70}")
            print(f"  Parser: custom_parsers/{args.target}_parser.py")
            print(f"  Log: custom_parsers/{args.target}_generation_log.txt")
            print(f"{'='*70}\n")
            sys.exit(0)
        else:
            print(f"\n{'='*70}")
            print(f"‚ùå GENERATION FAILED")
            print(f"{'='*70}")
            print("  Check _debug_generated.py to see the last attempt")
            print(f"{'='*70}\n")
            sys.exit(1)
    
    except KeyboardInterrupt:
        print("\n\n‚ö† Interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
